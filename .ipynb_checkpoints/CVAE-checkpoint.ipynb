{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613ce9c8-c903-4c1e-91bc-db0f4e49b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "def get_training_samples(batch_size):\n",
    "  \"\"\"\n",
    "  Yields batches of randomly selected lines from the training data.\n",
    "\n",
    "  Args:\n",
    "      batch_size: The desired number of samples in each batch.\n",
    "\n",
    "  Yields:\n",
    "      A tuple containing:\n",
    "          labels: A list of labels for the samples in the batch.\n",
    "          inputs: A list of input vectors for the samples in the batch.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(\"train.csv\") as file:\n",
    "    text = file.read()\n",
    "  textlines = text.strip().split(\"\\n\")\n",
    "\n",
    "  # Randomly shuffle the data lines for better training\n",
    "  random.shuffle(textlines)\n",
    "\n",
    "  while True:  # Loop indefinitely to keep yielding batches\n",
    "    batch_start = random.randint(0, len(textlines) - batch_size)\n",
    "    batch_end = min(batch_start + batch_size, len(textlines))  # Limit end to file length\n",
    "\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    for textline in textlines[batch_start:batch_end]:\n",
    "      cells = textline.split(\",\")\n",
    "      labels.append(int(cells[0]))\n",
    "      inputs.append([float(c) for c in cells[11:]])\n",
    "\n",
    "    yield labels, inputs\n",
    "\n",
    "def get_training_samples(batch_size):\n",
    "    with open(\"train.csv\") as file:\n",
    "        text = file.read()\n",
    "    textlines = text.strip().split(\"\\n\")\n",
    "    random.shuffle(textlines)\n",
    "    start = 0\n",
    "    while start < len(textlines):\n",
    "        labels = []\n",
    "        targets = []\n",
    "        inputs = []\n",
    "        end = start + batch_size\n",
    "        for textline in textlines[start:end]:\n",
    "            cells = textline.split(\",\")\n",
    "            labels.append(int(cells[0]))\n",
    "            targets.append([float(c) for c in cells[1:11]])\n",
    "            inputs.append([float(c) for c in cells[11:]])\n",
    "        #yield labels, targets, inputs\n",
    "        yield labels, inputs\n",
    "        start += batch_size\n",
    "\n",
    "def get_test_samples():\n",
    "    with open(\"test.csv\", \"r\") as file:\n",
    "        text = file.read()\n",
    "    textlines = text.strip().split(\"\\n\")\n",
    "    labels = []\n",
    "    targets = []\n",
    "    inputs = []\n",
    "    for textline in textlines:\n",
    "        cells = textline.split(\",\")\n",
    "        value = int(cells[0])\n",
    "        labels.append(int(cells[0]))\n",
    "        targets.append([float(c) for c in cells[1:11]])\n",
    "        inputs.append([float(c) for c in cells[11:]])\n",
    "    return labels, targets, inputs\n",
    "\n",
    "def plot_number(inputs):\n",
    "    line = \"\"\n",
    "    for p in inputs:\n",
    "        line += \".░▒▓█\"[round(p * 4)]\n",
    "        if len(line) > 27:\n",
    "            print(line)\n",
    "            line = \"\"\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Convert a list of numerical labels to one-hot encoded format.\n",
    "\n",
    "    Args:\n",
    "    labels (list or np.ndarray): List of numerical labels to be one-hot encoded.\n",
    "    num_classes (int): Total number of classes or unique labels.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: One-hot encoded matrix of shape (len(labels), num_classes).\n",
    "    \"\"\"\n",
    "    # Create an array of zeros with shape (len(labels), num_classes)\n",
    "    one_hot = np.zeros((len(labels), num_classes), dtype=int)\n",
    "\n",
    "    # Set the appropriate elements to 1\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620960d0-19dd-46f0-923a-803bf1eb909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(predictions):\n",
    "    m = max(predictions)\n",
    "    temp = [math.exp(p - m) for p in predictions]\n",
    "    total = sum(temp)\n",
    "    return [t / total for t in temp]\n",
    "\n",
    "def sigmoid(value):\n",
    "    # Clip value to avoid overflow in exp\n",
    "    clipped_value = np.clip(value, -500, 500)  # You can adjust these limits based on your actual value range\n",
    "    return 1 / (1 + np.exp(-clipped_value))\n",
    "    \n",
    "#def log_loss(activations, targets):\n",
    "#    losses = [-t * math.log(a) - (1 - t) * math.log(1 - a) for a, t in zip(activations, targets)]\n",
    "#    return sum(losses)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def log_loss(activations, targets):\n",
    "    # Clipping values to avoid math domain error\n",
    "    clipped_activations = [max(1e-15, min(a, 1 - 1e-15)) for a in activations]\n",
    "    losses = [-t * math.log(a) - (1 - t) * math.log(1 - a) for a, t in zip(clipped_activations, targets)]\n",
    "    return sum(losses)\n",
    "\n",
    "def clip_gradient(gradient, min_value= -0.2, max_value=0.2):\n",
    "    \"\"\"\n",
    "    Clip the gradient to a specified range.\n",
    "\n",
    "    :param gradient: The calculated gradient, which can be an array.\n",
    "    :param min_value: The minimum allowed value for the gradient.\n",
    "    :param max_value: The maximum allowed value for the gradient.\n",
    "    :return: The clipped gradient.\n",
    "    \"\"\"\n",
    "    return np.clip(gradient, min_value, max_value)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Weight Initialisation\n",
    "# -------------------------------\n",
    "\n",
    "def initialise_weight(in_channel, out_channel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    W = np.random.randn(in_channel, out_channel).astype(np.float32) * np.sqrt(2.0/(in_channel))\n",
    "    return W\n",
    "\n",
    "\n",
    "def initialise_bias(out_channel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    b = np.zeros(out_channel).astype(np.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Loss Functions\n",
    "# -------------------------------\n",
    "\n",
    "def BCELoss(x, y, derivative=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def _BCE_loss_forward(x, y):\n",
    "        loss = np.sum(- y * np.log(x + eps) + - (1 - y) * np.log((1 - x) + eps))\n",
    "        return loss\n",
    "\n",
    "    def _BCE_loss_derivative(x, y):\n",
    "        dloss = -y * (1 / (x + eps))\n",
    "        return dloss\n",
    "    \n",
    "    if derivative:\n",
    "        return _BCE_loss_derivative(x, y)\n",
    "    else:\n",
    "        return _BCE_loss_forward(x, y)\n",
    "\n",
    "\n",
    "def MSELoss(x, y, derivative=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def _MSE_loss_forward(x, y):\n",
    "        loss = (np.square(y - x)).mean()\n",
    "        return loss\n",
    "\n",
    "    def _MSE_loss_derivative(x, y):\n",
    "        dloss = 2 * (x - y)\n",
    "        return dloss\n",
    "    \n",
    "    if derivative:\n",
    "        return _MSE_loss_derivative(x, y)\n",
    "    else:\n",
    "        return _MSE_loss_forward(x, y)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Activation Functions\n",
    "# -------------------------------\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    #res = 1/(1+np.exp(-x))\n",
    "    clipped_value = np.clip(x, -1000, 1000)  # You can adjust these limits based on your actual value range\n",
    "    res = 1 / (1 + np.exp(-clipped_value))\n",
    "    if derivative:\n",
    "        return res*(1-res)\n",
    "    return res\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    res = x\n",
    "    if derivative:\n",
    "        return 1.0 * (res > 0)\n",
    "    else:\n",
    "        return res * (res > 0)   \n",
    "    \n",
    "def lrelu(x, alpha=0.01, derivative=False):\n",
    "    res = x\n",
    "    if derivative:\n",
    "        dx = np.ones_like(res)\n",
    "        dx[res < 0] = alpha\n",
    "        return dx\n",
    "    else:\n",
    "        return np.maximum(x, x*alpha, x)\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    res = np.tanh(x)\n",
    "    if derivative:\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "    return res\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Convert a list of numerical labels to one-hot encoded format.\n",
    "\n",
    "    Args:\n",
    "    labels (list or np.ndarray): List of numerical labels to be one-hot encoded.\n",
    "    num_classes (int): Total number of classes or unique labels.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: One-hot encoded matrix of shape (len(labels), num_classes).\n",
    "    \"\"\"\n",
    "    # Create an array of zeros with shape (len(labels), num_classes)\n",
    "    one_hot = np.zeros((len(labels), num_classes), dtype=int)\n",
    "\n",
    "    # Set the appropriate elements to 1\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# Example usage\n",
    "labels = [0, 1, 2, 3, 0, 2, 1]  # Example list of labels\n",
    "num_classes = 4  # Assuming 4 classes for this example\n",
    "\n",
    "one_hot_encoded = one_hot_encode(labels, num_classes)\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33aeb74c-6962-4532-9b45-6600da1aebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "input_count, hidden_count, mean_Count, variance_Count, hidden_cnt, output_count = 784, 262, 152, 152, 262, 784\n",
    "\n",
    "W1 = np.random.randn(input_count + 10, hidden_count).astype(np.float32) * np.sqrt(2.0/(input_count + 10))\n",
    "b1 = np.zeros(hidden_count).astype(np.float32)\n",
    "\n",
    "mu_Weight = np.random.randn(hidden_count, mean_Count).astype(np.float32) * np.sqrt(2.0/(hidden_count))\n",
    "mu_Bias = np.zeros(mean_Count).astype(np.float32)\n",
    "\n",
    "sd_Weight = np.random.randn(hidden_count, variance_Count).astype(np.float32) * np.sqrt(2.0/(hidden_count))\n",
    "sd_Bias = np.zeros(variance_Count).astype(np.float32)\n",
    "\n",
    "W2 = np.random.randn(mean_Count + 10, hidden_cnt).astype(np.float32) * np.sqrt(2.0/(mean_Count))\n",
    "b2 = np.zeros(hidden_cnt).astype(np.float32)\n",
    "\n",
    "W3 = np.random.randn(hidden_cnt, output_count).astype(np.float32) * np.sqrt(2.0/(hidden_cnt))\n",
    "b3 = np.zeros(output_count).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57e6c037-9509-43c6-9f18-85d52914f9f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.momentum = [np.zeros_like(param) for param in parameters]\n",
    "        self.velocity = [np.zeros_like(param) for param in parameters]\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, parameters, grads):\n",
    "        self.t += 1  # Update time step\n",
    "        updated_params = []\n",
    "        for i, (param, grad) in enumerate(zip(parameters, grads)):\n",
    "            self.momentum[i] = self.beta1 * self.momentum[i] + (1 - self.beta1) * grad\n",
    "            self.velocity[i] = self.beta2 * self.velocity[i] + (1 - self.beta2) * np.power(grad, 2)\n",
    "            \n",
    "            m_hat = self.momentum[i] / (1 - np.power(self.beta1, self.t))\n",
    "            v_hat = self.velocity[i] / (1 - np.power(self.beta2, self.t))\n",
    "            \n",
    "            param_update = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "            parameters[i] -= param_update\n",
    "            updated_params.append(parameters[i])\n",
    "        return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35c9c354-2f8c-41d2-92c4-b3f30fed3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = [0.0] * 10\n",
    "velocity = [0.0] * 10\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9999\n",
    "#t = 0\n",
    "\n",
    "def optimise(grads):\n",
    "    t = 0\n",
    "    t += 1\n",
    "        # Calculate gradient with momentum and velocity\n",
    "    for i, grad in enumerate(grads):\n",
    "        #print(i)\n",
    "        momentum[i] = beta1 * momentum[i] + (1 - beta1) * grad\n",
    "        velocity[i] = beta2 * velocity[i] + (1 - beta2) * np.power(grad, 2)\n",
    "        m_h = momentum[i] / (1 - (beta1 ** t))\n",
    "        v_h = velocity[i] /  (1 - (beta2 ** t))\n",
    "        grads[i] = m_h / np.sqrt(v_h + eps)\n",
    "\n",
    "        #print('---------encoder------------',grad_W0.shape, grad_b0.shape, grad_W_mu.shape, grad_b_mu.shape, grad_W_logvar.shape, grad_b_logvar.shape)\n",
    "        W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias = grads\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a4df0-c444-449c-9707-cc646d04c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "beta = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    index = 0\n",
    "    for labels, inputs in get_training_samples(batch_size):\n",
    "        index += 1\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        targets = inputs.copy()\n",
    "        #print(labels)\n",
    "        one_hot_encoded = np.array(one_hot_encode(labels, 10))\n",
    "        #print(one_hot_encoded)\n",
    "        inputConcatenate =  np.concatenate([inputs, one_hot_encoded], axis = 1)\n",
    "        #print(inputs.shape, inputConcatenate.shape)\n",
    "\n",
    "        hValue = np.dot(inputConcatenate, W1) + b1\n",
    "        hNeuron = sigmoid(hValue)\n",
    "\n",
    "        #mean and standard deviation\n",
    "        muNeuron = (np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "        sdNeuron = (np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "        #print(hNeuron.shape, sdNeuron.shape, muNeuron.shape)      \n",
    "        randomSample =  np.random.standard_normal(size=(batch_size, muNeuron.shape[1]))\n",
    "        zSpace = muNeuron + np.exp(sdNeuron * 0.5) * randomSample\n",
    "        zSpace = np.concatenate([zSpace, one_hot_encoded], axis = 1)\n",
    "        #print(zSpace.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        hdValue = np.dot(zSpace, W2) + b2\n",
    "        hdNeuron = sigmoid(hdValue)\n",
    "        oValue = np.dot(hdNeuron, W3) + b3\n",
    "        output = sigmoid(oValue)\n",
    "        \n",
    "        # Decoder Backpropagation\n",
    "        d_error = MSELoss(output, inputs, derivative=True)\n",
    "        d_pred = sigmoid(oValue, derivative=True)\n",
    "        d_output = d_error * d_pred\n",
    "        \n",
    "        #d_b3 = d_output\n",
    "        d_b3 = np.sum(d_output, axis=0)\n",
    "        #print(d_b3.shape)\n",
    "        d_W3 = np.dot(hdNeuron.T, d_output)\n",
    "\n",
    "        d_hdNeuron = np.dot(d_output, W3.T) * sigmoid(hdValue, derivative=True)\n",
    "        d_b2 = np.sum(d_hdNeuron, axis=0)\n",
    "        #d_b2 = d_hdNeuron\n",
    "        d_W2 = np.dot(zSpace.T, d_hdNeuron)\n",
    "        \n",
    "        d_zSpace = np.dot(d_hdNeuron, W2.T)\n",
    "        d_zSpace = d_zSpace[:, :152]\n",
    "        #print('---------',d_zSpace.shape)\n",
    "\n",
    "        # Encoder Backpropagation \n",
    "        d_muNeuron = d_zSpace\n",
    "        d_muBias = np.sum(d_muNeuron, axis = 0)\n",
    "        d_muWeight = np.dot(hNeuron.T, d_muNeuron) \n",
    "\n",
    "        d_sdNeuron = d_zSpace * np.exp(sdNeuron * .5) * .5 * randomSample\n",
    "        #d_sdBias = d_sdNeuron\n",
    "        d_sdBias = np.sum(d_sdNeuron, axis = 0)\n",
    "        d_sdWeight = np.dot(hNeuron.T, d_sdNeuron)\n",
    "\n",
    "        hNeuronDerivative = sigmoid(hValue, derivative=True)\n",
    "        dhNeuron = hNeuronDerivative * (np.dot(d_muBias, mu_Weight.T) + np.dot(d_sdNeuron, sd_Weight.T))  \n",
    "        db1 = np.sum(dhNeuron, axis = 0)\n",
    "        #db1 = dhNeuron\n",
    "        dW1 = np.dot(inputConcatenate.T, dhNeuron)\n",
    "        \n",
    "        dk1_muNeuron = .5 * 2 * muNeuron\n",
    "        dkl_muBias = np.sum(dk1_muNeuron, axis = 0)\n",
    "        #dkl_muBias = dk1_muNeuron\n",
    "        dkl_muWeight = np.dot(hNeuron.T, dk1_muNeuron) * beta\n",
    "\n",
    "        dk1_sdNeuron = .5 * (np.exp(sdNeuron) - 1)\n",
    "        dkl_sdBias = np.sum(dk1_sdNeuron, axis = 0)\n",
    "        #dkl_sdBias = dk1_sdNeuron\n",
    "        dkl_sdWeight = np.dot(hNeuron.T, dk1_sdNeuron) * beta\n",
    "  \n",
    "        dkl_hNeuron = hNeuronDerivative * (np.dot(dk1_muNeuron, mu_Weight.T) + np.dot(dk1_sdNeuron, sd_Weight.T))\n",
    "        dkl_W1 = np.dot(inputConcatenate.T, dkl_hNeuron)\n",
    "        dkl_b1 = np.sum(dkl_hNeuron, axis = 0)\n",
    "        #dkl_b1 = dkl_hNeuron\n",
    "\n",
    "        grad_b_logvar = dkl_sdBias + d_sdBias\n",
    "        grad_W_logvar = dkl_sdWeight + d_sdWeight\n",
    "        grad_b_mu = dkl_muBias + d_muBias\n",
    "        grad_W_mu = dkl_muWeight + d_muWeight\n",
    "        grad_b1 = dkl_b1 + db1\n",
    "        grad_W1 = dkl_W1 + dW1   \n",
    "        #print(grad_W1.shape)\n",
    "\n",
    "        d_W3 = clip_gradient(d_W3)\n",
    "        d_b3 = clip_gradient(d_b3)\n",
    "        d_W2 = clip_gradient(d_W2)\n",
    "        d_b2 = clip_gradient(d_b2)\n",
    "        grad_W1 = clip_gradient(grad_W1)\n",
    "        grad_b1 = clip_gradient(grad_b1)\n",
    "        grad_W_mu = clip_gradient(grad_W_mu)\n",
    "        grad_b_mu = clip_gradient(grad_b_mu)\n",
    "        grad_W_logvar = clip_gradient(grad_W_logvar)\n",
    "        grad_b_logvar = clip_gradient(grad_b_logvar)\n",
    "\n",
    "        #grads = [W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias]\n",
    "        #optimise(grads)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W3 -= learning_rate * d_W3\n",
    "        b3 -= learning_rate * d_b3\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        mu_Weight -= learning_rate * grad_W_mu\n",
    "        mu_Bias -= learning_rate * grad_b_mu\n",
    "        sd_Weight -= learning_rate * grad_W_logvar\n",
    "        sd_Bias -= learning_rate * grad_b_logvar\n",
    "\n",
    "        klbloss = -0.5 * np.sum(1 + sdNeuron - muNeuron**2 - np.exp(2 * sdNeuron)) / batch_size\n",
    "\n",
    "        #loss = -0.5 * np.sum(1 + self.latent_logvar - self.latent_mu**2 - np.exp(self.latent_logvar)) / (self.batch_size * self.latent_dim)\n",
    "        rec_loss = MSELoss(output, inputs)\n",
    "        totalLoss = rec_loss + klbloss\n",
    "        #print('---------------', index, '---------------')\n",
    "        print(totalLoss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1851980e-f0ee-46c6-975d-5727390dc110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\n",
      "............................\n",
      "............▒▓░.............\n",
      "...........░█▓▒.............\n",
      "...........█▒...............\n",
      "..........▒█................\n",
      "..........█▒................\n",
      "..........█░................\n",
      ".........░█.................\n",
      ".........▓█.................\n",
      ".........▓▓......░▒▒▒░......\n",
      ".........█░.....▒██▓▓█░.....\n",
      ".........█░....▓█▒...░▓.....\n",
      "........░█....▒█░.....█.....\n",
      "........░█....█░......▓░....\n",
      "........░█░..▒█.......▓▒....\n",
      ".........█▒..░█░......▓░....\n",
      ".........▓█..░█░.....▒█.....\n",
      ".........░█▒..▒█▒▒▒░░█░.....\n",
      "..........▓█▒..░▒▒▒▒▓░......\n",
      "...........▒██▒▒▒▓█▓░.......\n",
      "............░▒▓█▓▒░.........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "................░░░.........\n",
      ".............░░░░░░░░.......\n",
      "............░░░▒▒▒░░░.......\n",
      "...........░░░▒▒▒▒░░........\n",
      "..........░░░▒▒▒▒░░.........\n",
      ".........░░░▒▒▒▒░░..........\n",
      ".........░░▒▒▒▒░░...........\n",
      "........░░░▒▒▒▒░............\n",
      "........░░▒▒▓▒░░............\n",
      ".......░░░▒▓▒▒░..░░░░░......\n",
      ".......░░▒▓▓▒░░░░░░░░░░.....\n",
      ".......░░▒▓▓▒░░▒▒▒▒▒▒░░.....\n",
      ".......░▒▓▓▒▒▒▒▒▓▓▒▒▒▒░.....\n",
      "......░░▒▓▓▒▒▒▒▓▓▒▒▒▒▒░░....\n",
      "......░░▒▓▓▓▒▒▒▒▒▒▒▓▒▒░.....\n",
      "......░░▒▓▓▓▒▒▒▒▒▒▒▓▒▒░.....\n",
      ".......░▒▓▓▓▓▒▒▒▒▓▓▓▒░░.....\n",
      ".......░▒▓▓▓▓▓▓▓▓▓▓▒░░......\n",
      "........░▒▓▓▓▓▓▓▓▒▒░░.......\n",
      "........░░▒▒▓▓▓▓▒░░░........\n",
      "..........░░░░░░░...........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      ".............░▓█░░▒.........\n",
      "............░████▓█▓........\n",
      "...........▒▓██▓█▒▓█▓.......\n",
      "..........▒██▓░▒▓.▒██░......\n",
      ".........▒██▓░.....░▓█░.....\n",
      "........░███░.......▒█▓.....\n",
      "........███▒.........▒█.....\n",
      ".......░██▒..........░█░....\n",
      ".......░██...........░█.....\n",
      ".......▒██...........▒█░....\n",
      ".......▓██...........░█░....\n",
      "......░██░..........░█▓.....\n",
      "......░██░..........▒█▓.....\n",
      "......░██░.........░██░.....\n",
      ".......░██........░▓█▓......\n",
      "........▓█▓.......▓██.......\n",
      "........▒██▒....░▒██▒.......\n",
      ".........▒██▓▓▒▓███░........\n",
      "..........▒██████▓░.........\n",
      "............░░▓▒░...........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      ".............░░░░░░.........\n",
      "............░░▒▒▒▒░░░.......\n",
      "..........░░▒▒▓▓▓▓▒▒░░......\n",
      ".........░░▒▒▓▓▓▓▓▓▒▒░░.....\n",
      "........░░▒▒▓▓▓▓▒▒▒▒▒▒░.....\n",
      "........░▒▒▓▓▓▒▒░░▒▒▒▒░░....\n",
      ".......░░▒▓▓▒▒░░░░░▒▒▒▒░....\n",
      ".......░▒▒▓▒▒░░..░░▒▒▓▒░....\n",
      "......░░▒▒▓▒░░....░▒▒▓▒░....\n",
      "......░▒▒▒▒░░.....░░▒▓▒░....\n",
      "......░▒▒▒▒░......░▒▒▒▒░....\n",
      ".....░░▒▒▒░░......░▒▒▒░░....\n",
      ".....░▒▒▒▒░......░░▒▒▒░.....\n",
      ".....░▒▒▒▒░.....░░▒▒▒▒░.....\n",
      ".....░▒▒▒▒░...░░░▒▒▒▒░░.....\n",
      ".....░▒▒▒▒░░░░░▒▒▒▒▒░░......\n",
      ".....░░▒▓▓▒▒▒▒▒▒▒▒▒░░.......\n",
      "......░▒▒▓▓▓▓▓▓▓▒▒░░........\n",
      "......░░▒▒▓▓▓▓▓▒▒░░.........\n",
      "........░░▒▒▒▒▒░░░..........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      ".................░▒▒▒█▓▓░...\n",
      "........▒▓▓█▓▓▓█████████▒...\n",
      ".......░███████▓▓▓▓░░░██░...\n",
      ".......▓█▓▓█▓░........░.....\n",
      ".....░██▓.░█................\n",
      ".....▒██▓▓░░░...............\n",
      ".....░███████▒░.............\n",
      "..........░▓███▒░...........\n",
      "............░▒███▒..........\n",
      "..............░▓██▓.........\n",
      "................▒██░........\n",
      ".................▒█▓........\n",
      ".................░██░.......\n",
      ".................▓██........\n",
      "...............░▓██▒........\n",
      "..............░▓██▒.........\n",
      ".......░.....▓███▒..........\n",
      ".......▓▒░░▓███▓░...........\n",
      ".......▓██████▒.............\n",
      "........▒▓█▓▒...............\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............░░░░░░░░░░░░....\n",
      "..........░░░▒▒▒▒▒▒▒░░░░░...\n",
      ".........░░▒▒▒▒▒▒▒▒▒▒▒░░░...\n",
      ".........░░▒▒▒▒▒▒▒▒▒░░░░░...\n",
      "........░░▒▒▒▒▒░░░░░░░░.....\n",
      "........░░▒▓▒▒░░░░░░........\n",
      "........░▒▓▓▓▒░░░░..........\n",
      "........░▒▓▓▓▒▒░░░░.........\n",
      "........░▒▓▓▓▒▒▒░░░░........\n",
      "........░▒▓▓▓▒▒▒▒▒░░░.......\n",
      "........░░▒▒▒░░▒▒▒░░░.......\n",
      ".........░░░░░░░▒▒▒░░.......\n",
      "..............░░▒▒▒░░.......\n",
      ".......░░....░░▒▒▒▒░░.......\n",
      "......░░░░░░░░░▒▒▒▒░░.......\n",
      "......░░▒░░░░░▒▒▒▒░░........\n",
      "......░░▒▒▒▒▒▒▒▒▒▒░░........\n",
      ".......░░▒▒▒▒▓▓▒▒░░.........\n",
      "........░░▒▒▒▒▒▒░░..........\n",
      ".........░░░░░░░............\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "..................░▓░.......\n",
      "..................▓█▓.......\n",
      "..................▓█▓.......\n",
      ".......░▓░........▓█▒.......\n",
      ".......▓█░........██▒.......\n",
      ".......▒█▓.......░██░.......\n",
      ".......▒█▓.......░██........\n",
      ".......▒██.......░██........\n",
      ".......██▒.......▒██........\n",
      ".....░▒█▓░.......▒██........\n",
      "....░███▓░.░.....▒██........\n",
      "...▒██████▓█▓▓▓▓▓██▓........\n",
      "...▓███▓█▓█████████▓........\n",
      "...░░......░░░░░▓██▓........\n",
      "................▒██▓........\n",
      "................▒██▓........\n",
      "................▒██▓........\n",
      "................░▓█▓........\n",
      ".................▒█▓........\n",
      ".................▒▓░........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      ".....................░......\n",
      "..........░░░....░░░░░░.....\n",
      ".........░░░░░..░░░░░░░.....\n",
      "........░░░▒░░░.░░░▒░░░.....\n",
      "........░░▒▒░░..░░▒▒░░......\n",
      ".......░░▒▒▒░░..░▒▒▒░░......\n",
      ".......░▒▒▒▒░..░░▒▒▒░░......\n",
      ".......░▒▓▒▒░..░▒▓▓▒░.......\n",
      "......░░▒▓▓▒░░░░▒▓▓▒░.......\n",
      "......░▒▓▓▓▒▒▒▒▒▓▓▓▒░░......\n",
      "......░▒▓▓▓▓▓▒▓▓▓▓▓▒░░......\n",
      "......░▒▒▒▒▒▒▒▓▓▓▓▒░░.......\n",
      "......░░░░░░░▒▒▓▓▒▒░░.......\n",
      "........░░░░░░▒▒▒▒░░........\n",
      "............░░▒▒▒░░.........\n",
      "............░░░▒░░░.........\n",
      "...........░░░░░░░░.........\n",
      "...........░░░░░░░░.........\n",
      "...........░░░░░░░░.........\n",
      "............░░░░░░..........\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "..........▒████▓............\n",
      "........░▓███▓███...........\n",
      ".......░███░░.░▓█░..........\n",
      ".......▓█▓░.....██░.........\n",
      "......▒██░.......▒░.........\n",
      "......▓█▓...................\n",
      "......██▒...................\n",
      "......██▒...................\n",
      "......▒█▓▒░.░▒▒▓▓▓▓▒░.......\n",
      ".......▓███▒█████████.......\n",
      ".......░▓▒▒▒▒▒▒░░░░▒█░......\n",
      "...................██▒......\n",
      "...................██▒......\n",
      "..................░██░......\n",
      "..................▒██░......\n",
      "..................▓█▓.......\n",
      "..................▓█▓.......\n",
      "..................▓█▓.......\n",
      "..................▒██░......\n",
      "...................▓▒.......\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............░░░░░░░.........\n",
      ".........░░░▒▒▓▓▓▒░░........\n",
      "........░░▒▒▓▓▓▓▒▒▒░░.......\n",
      ".......░░▒▒▒▒▒░░▒▒▒░░.......\n",
      "......░░▒▒▒▒░░░░░▒▒▒░.......\n",
      ".....░░▒▒▒▒░░..░▒▒▒▒░░......\n",
      ".....░░▒▒▒▒░░░░░▒▓▒▒░░......\n",
      ".....░░▒▒▒▒░░░░▒▓▓▓▒░░......\n",
      ".....░░▒▒▒▒▒▒▒▒▒▓▓▒░░.......\n",
      "......░░▒▒▒▒▒▒▒▒▓▓▒░........\n",
      "......░░░░▒▒░░▒▒▓▒░░........\n",
      "........░░░░░░░▒▓▒░░........\n",
      "............░░▒▒▒▒░.........\n",
      ".............░▒▒▒░░░........\n",
      "............░░░░░░░░........\n",
      "............░░░░░░░░........\n",
      "...........░░░░░░░░░░.......\n",
      "...........░░░░░░░░░░.......\n",
      "..........░░░░░░░░░░░.......\n",
      "..........░░░░░░░░░░........\n",
      "............................\n",
      "............................\n"
     ]
    }
   ],
   "source": [
    "labels, targets, inputs = get_test_samples()\n",
    "\n",
    "hNeuron = sigmoid(np.dot(np.concatenate([inputs,one_hot_encode(labels, 10)], axis = 1), W1) + b1)\n",
    "#print(pred_h[3].shape)\n",
    "\n",
    "muNeuron = (np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "sdNeuron = (np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "\n",
    "#print(muNeuron.shape)\n",
    "\n",
    "randomSample = np.random.standard_normal(size=(1000, muNeuron.shape[1]))\n",
    "zSpace = muNeuron + np.exp(sdNeuron * 0.5) * randomSample\n",
    "\n",
    "hdNeuron = sigmoid(np.dot(np.concatenate([zSpace,one_hot_encode(labels, 10)], axis = 1), W2) + b2)\n",
    "output = sigmoid(np.dot(hdNeuron, W3) + b3)\n",
    "\n",
    "\n",
    "for v, i in zip(inputs[100:105], output[100:105]):\n",
    "    #print(v)\n",
    "    plot_number(v)\n",
    "    plot_number(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f2d90-bb72-4bf8-a2bb-b1387d2fffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f0a7cf0c-3a15-4331-ad91-145f50a005f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = [0.0] * 10\n",
    "velocity = [0.0] * 10\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9999\n",
    "#t = 0\n",
    "\n",
    "def optimise(grads):\n",
    "    t = 0\n",
    "    t += 1\n",
    "        # Calculate gradient with momentum and velocity\n",
    "    for i, grad in enumerate(grads):\n",
    "        #print(i)\n",
    "        momentum[i] = beta1 * momentum[i] + (1 - beta1) * grad\n",
    "        velocity[i] = beta2 * velocity[i] + (1 - beta2) * np.power(grad, 2)\n",
    "        m_h = momentum[i] / (1 - (beta1 ** t))\n",
    "        v_h = velocity[i] /  (1 - (beta2 ** t))\n",
    "        grads[i] = m_h / np.sqrt(v_h + eps)\n",
    "\n",
    "        #print('---------encoder------------',grad_W0.shape, grad_b0.shape, grad_W_mu.shape, grad_b_mu.shape, grad_W_logvar.shape, grad_b_logvar.shape)\n",
    "        W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias = grads\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "03b54bd0-1fea-4ac5-8144-11ec757de3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 1000\n",
    "learning_rate = 0.0001\n",
    "input_count, hidden_count, mean_Count, variance_Count, hidden_cnt, output_count = 784, 262, 200, 200, 262, 784\n",
    "\n",
    "W1 = np.random.randn(input_count, hidden_count).astype(np.float32) * np.sqrt(2.0/(input_count))\n",
    "b1 = np.zeros(hidden_count).astype(np.float32)\n",
    "\n",
    "mu_Weight = np.random.randn(hidden_count, mean_Count).astype(np.float32) * np.sqrt(2.0/(hidden_count))\n",
    "mu_Bias = np.zeros(mean_Count).astype(np.float32)\n",
    "\n",
    "sd_Weight = np.random.randn(hidden_count, variance_Count).astype(np.float32) * np.sqrt(2.0/(hidden_count))\n",
    "sd_Bias = np.zeros(variance_Count).astype(np.float32)\n",
    "\n",
    "W2 = np.random.randn(mean_Count, hidden_cnt).astype(np.float32) * np.sqrt(2.0/(mean_Count))\n",
    "b2 = np.zeros(hidden_cnt).astype(np.float32)\n",
    "\n",
    "W3 = np.random.randn(hidden_cnt, output_count).astype(np.float32) * np.sqrt(2.0/(hidden_cnt))\n",
    "b3 = np.zeros(output_count).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e938f5-4378-4083-9ab6-7a0076b3f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    index = 0\n",
    "    for labels, inputs in get_training_samples(batch_size):\n",
    "        index += 1\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        targets = inputs.copy()\n",
    "\n",
    "        hValue = np.dot(inputs, W1) + b1\n",
    "        hNeuron = sigmoid(hValue)\n",
    "\n",
    "        #mean and standard deviation\n",
    "        muNeuron = (np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "        sdNeuron = (np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "        #print(hNeuron.shape, sdNeuron.shape, muNeuron.shape)      \n",
    "        randomSample =  np.random.standard_normal(size=(batch_size, muNeuron.shape[1]))\n",
    "        zSpace = muNeuron + np.exp(sdNeuron * 0.5) * randomSample\n",
    "        #print(zSpace.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        hdValue = np.dot(zSpace, W2) + b2\n",
    "        hdNeuron = sigmoid(hdValue)\n",
    "        oValue = np.dot(hdNeuron, W3) + b3\n",
    "        output = sigmoid(oValue)\n",
    "        \n",
    "        # Decoder Backpropagation\n",
    "        d_error = MSELoss(output, inputs, derivative=True)\n",
    "        d_pred = sigmoid(oValue, derivative=True)\n",
    "        d_output = d_error * d_pred\n",
    "        \n",
    "        #d_b3 = d_output\n",
    "        d_b3 = np.sum(d_output, axis=0)\n",
    "        #print(d_b3.shape)\n",
    "        d_W3 = np.dot(hdNeuron.T, d_output)\n",
    "\n",
    "        d_hdNeuron = np.dot(d_output, W3.T) * sigmoid(hdValue, derivative=True)\n",
    "        d_b2 = np.sum(d_hdNeuron, axis=0)\n",
    "        #d_b2 = d_hdNeuron\n",
    "        d_W2 = np.dot(zSpace.T, d_hdNeuron)\n",
    "        \n",
    "        d_zSpace = np.dot(d_hdNeuron, W2.T)\n",
    "        print('---------',d_zSpace.shape)\n",
    "\n",
    "        # Encoder Backpropagation \n",
    "        d_muNeuron = d_zSpace\n",
    "        d_muBias = np.sum(d_muNeuron, axis = 0)\n",
    "        d_muWeight = np.dot(hNeuron.T, d_muNeuron) \n",
    "\n",
    "        d_sdNeuron = d_zSpace * np.exp(sdNeuron * .5) * .5 * randomSample\n",
    "        #d_sdBias = d_sdNeuron\n",
    "        d_sdBias = np.sum(d_sdNeuron, axis = 0)\n",
    "        d_sdWeight = np.dot(hNeuron.T, d_sdNeuron)\n",
    "\n",
    "        hNeuronDerivative = sigmoid(hValue, derivative=True)\n",
    "        dhNeuron = hNeuronDerivative * (np.dot(d_muBias, mu_Weight.T) + np.dot(d_sdNeuron, sd_Weight.T))  \n",
    "        db1 = np.sum(dhNeuron, axis = 0)\n",
    "        #db1 = dhNeuron\n",
    "        dW1 = np.dot(inputs.T, dhNeuron)\n",
    "        \n",
    "        dk1_muNeuron = .5 * 2 * muNeuron\n",
    "        dkl_muBias = np.sum(dk1_muNeuron, axis = 0)\n",
    "        #dkl_muBias = dk1_muNeuron\n",
    "        dkl_muWeight = np.dot(hNeuron.T, dk1_muNeuron) \n",
    "\n",
    "        dk1_sdNeuron = .5 * (np.exp(sdNeuron) - 1)\n",
    "        dkl_sdBias = np.sum(dk1_sdNeuron, axis = 0)\n",
    "        #dkl_sdBias = dk1_sdNeuron\n",
    "        dkl_sdWeight = np.dot(hNeuron.T, dk1_sdNeuron)\n",
    "  \n",
    "        dkl_hNeuron = hNeuronDerivative * (np.dot(dk1_muNeuron, mu_Weight.T) + np.dot(dk1_sdNeuron, sd_Weight.T))\n",
    "        dkl_W1 = np.dot(inputs.T, dkl_hNeuron)\n",
    "        dkl_b1 = np.sum(dkl_hNeuron, axis = 0)\n",
    "        #dkl_b1 = dkl_hNeuron\n",
    "\n",
    "        grad_b_logvar = dkl_sdBias + d_sdBias\n",
    "        grad_W_logvar = dkl_sdWeight + d_sdWeight\n",
    "        grad_b_mu = dkl_muBias + d_muBias\n",
    "        grad_W_mu = dkl_muWeight + d_muWeight\n",
    "        grad_b1 = dkl_b1 + db1\n",
    "        grad_W1 = dkl_W1 + dW1     \n",
    "\n",
    "        grads = [W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias]\n",
    "        optimise(grads)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W3 -= learning_rate * d_W3\n",
    "        b3 -= learning_rate * d_b3\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        mu_Weight -= learning_rate * grad_W_mu\n",
    "        mu_Bias -= learning_rate * grad_b_mu\n",
    "        sd_Weight -= learning_rate * grad_W_logvar\n",
    "        sd_Bias -= learning_rate * grad_b_logvar\n",
    "\n",
    "        klbloss = -0.5 * np.sum(1 + sdNeuron - muNeuron**2 - np.exp(2 * sdNeuron)) / batch_size\n",
    "\n",
    "        #loss = -0.5 * np.sum(1 + self.latent_logvar - self.latent_mu**2 - np.exp(self.latent_logvar)) / (self.batch_size * self.latent_dim)\n",
    "        rec_loss = MSELoss(output, inputs)\n",
    "        totalLoss = rec_loss + klbloss\n",
    "        #print('---------------', index, '---------------')\n",
    "        print(totalLoss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf82d1-a80b-4340-aa36-9222d1b981d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4d7f9-9f9f-4f8b-bfc6-0e7d28459f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a70723-93c2-4e0b-962f-f50d1d84d803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba92519-0798-478f-b884-5bad6b847e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ff390-ff48-485a-a6f2-468cdc37f74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313757f-1690-4d2e-984d-5ede5cce4002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02421f6a-03e5-4718-b74f-e86246134ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542e46b-0d1b-4729-a405-7a7bb0e65477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c831686-0ade-4cd7-991a-0e4a00da2906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a3635-c159-46b0-83b5-40bb302ab35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6224d-84ba-4532-902b-e65707476203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c2c8b-e4ee-4258-9485-944ad6484692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e4934-544f-4515-a26e-98a2bb8d661e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e557eb3-3da8-455b-a670-152fac40d866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eec3ce-28da-475f-8cc1-9c91aea16b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "58236f6f-3398-4bba-bde4-f97be2df790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = [0.0] * 10\n",
    "velocity = [0.0] * 10\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9999\n",
    "#t = 0\n",
    "\n",
    "def optimise(grads):\n",
    "    t = 0\n",
    "    t += 1\n",
    "        # Calculate gradient with momentum and velocity\n",
    "    for i, grad in enumerate(grads):\n",
    "        #print(i)\n",
    "        momentum[i] = beta1 * momentum[i] + (1 - beta1) * grad\n",
    "        velocity[i] = beta2 * velocity[i] + (1 - beta2) * np.power(grad, 2)\n",
    "        m_h = momentum[i] / (1 - (beta1 ** t))\n",
    "        v_h = velocity[i] /  (1 - (beta2 ** t))\n",
    "        grads[i] = m_h / np.sqrt(v_h + eps)\n",
    "\n",
    "        #print('---------encoder------------',grad_W0.shape, grad_b0.shape, grad_W_mu.shape, grad_b_mu.shape, grad_W_logvar.shape, grad_b_logvar.shape)\n",
    "        W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias = grads\n",
    "\n",
    "        # Update weights and biases\n",
    "        W3 -= learning_rate *  np.sum(d_W3, axis=0)\n",
    "        b3 -= learning_rate * np.sum(d_b3, axis=0)\n",
    "        W2 -= learning_rate * np.sum(d_W2, axis=0)\n",
    "        b2 -= learning_rate * np.sum(d_b2, axis=0)\n",
    "        W1 -= learning_rate * np.sum(grad_W1, axis=0)\n",
    "        b1 -= learning_rate * np.sum(grad_b1, axis=0)\n",
    "        mu_Weight -= learning_rate * np.sum(grad_W_mu, axis= 0)\n",
    "        mu_Bias -= learning_rate * np.sum(grad_b_mu, axis = 0)\n",
    "        sd_Weight -= learning_rate * np.sum(grad_W_logvar, axis = 0)\n",
    "        sd_Bias -= learning_rate * np.sum(grad_b_logvar, axis = 0)\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8f3245ed-80c2-4bd6-80f2-4945494393c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10501941420016997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/166735951.py:100: RuntimeWarning: overflow encountered in exp\n",
      "  res = 1 / (1 + np.exp(-clipped_value))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14272593.363857418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3755677773.py:28: RuntimeWarning: overflow encountered in subtract\n",
      "  W1 -= learning_rate * np.sum(grad_W1, axis=0)\n",
      "/tmp/ipykernel_82/3755677773.py:29: RuntimeWarning: overflow encountered in subtract\n",
      "  b1 -= learning_rate * np.sum(grad_b1, axis=0)\n",
      "/tmp/ipykernel_82/3755677773.py:32: RuntimeWarning: overflow encountered in subtract\n",
      "  sd_Weight -= learning_rate * np.sum(grad_W_logvar, axis = 0)\n",
      "/tmp/ipykernel_82/3755677773.py:33: RuntimeWarning: overflow encountered in subtract\n",
      "  sd_Bias -= learning_rate * np.sum(grad_b_logvar, axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.637032573737365e+132\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[315], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m d_W3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(hdNeuron\u001b[38;5;241m.\u001b[39mT, d_output)\n\u001b[1;32m     40\u001b[0m d_b3 \u001b[38;5;241m=\u001b[39m d_output\n\u001b[0;32m---> 41\u001b[0m d_W3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdNeuron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39mexpand_dims(d_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     43\u001b[0m drelu0 \u001b[38;5;241m=\u001b[39m relu(hdValue, derivative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m d_b2 \u001b[38;5;241m=\u001b[39m d_b3\u001b[38;5;241m.\u001b[39mdot(W3\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m drelu0\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mexpand_dims\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    index = 0\n",
    "    for labels, inputs in get_training_samples(batch_size):\n",
    "        index += 1\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        targets = inputs.copy()\n",
    "\n",
    "        hValue = np.dot(inputs, W1) + b1\n",
    "        hNeuron = relu(hValue)\n",
    "\n",
    "        #mean and standard deviation\n",
    "        muNeuron = (np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "        sdNeuron = (np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "        #print(hNeuron.shape, sdNeuron.shape, muNeuron.shape)      \n",
    "        randomSample =  np.random.standard_normal(size=(batch_size, muNeuron.shape[1]))\n",
    "        zSpace = muNeuron + np.exp(sdNeuron * 0.5) * randomSample\n",
    "        #print(zSpace.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        hdValue = np.dot(zSpace, W2) + b2\n",
    "        hdNeuron = relu(hdValue)\n",
    "        oValue = np.dot(hdNeuron, W3) + b3\n",
    "        output = sigmoid(oValue)\n",
    "\n",
    "        #output = np.reshape(hdNeuron, (batch_size, 131))\n",
    "        \n",
    "        # Decoder Backpropagation\n",
    "        d_error = MSELoss(output, inputs, derivative=True)\n",
    "        d_pred = sigmoid(oValue, derivative=True)\n",
    "        d_output = d_error * d_pred\n",
    "        \n",
    "        #d_b3 = d_output\n",
    "        d_b3 = np.sum(d_output, axis=0)\n",
    "        #print(d_b3.shape)\n",
    "        d_W3 = np.dot(hdNeuron.T, d_output)\n",
    "\n",
    "        d_b3 = d_output\n",
    "        d_W3 = np.matmul(np.expand_dims(hdNeuron, axis=-1), np.expand_dims(d_output, axis=1))\n",
    "\n",
    "        drelu0 = relu(hdValue, derivative=True)\n",
    "        d_b2 = d_b3.dot(W3.T) * drelu0\n",
    "        d_W2 = np.matmul(np.expand_dims(zSpace, axis=-1), np.expand_dims(d_b2, axis=1))\n",
    "\n",
    "        d_zSpace = d_b2.dot(W2.T)\n",
    "\n",
    "        # Encoder Backpropagation \n",
    "        d_muBias = d_zSpace\n",
    "        #d_muBias = np.sum(d_muNeuron, axis = 0)\n",
    "        d_muWeight = np.matmul(np.expand_dims(hNeuron, axis=-1), np.expand_dims(d_muBias, axis=1))        \n",
    "\n",
    "        d_sdBias = d_zSpace * np.exp(sdNeuron * .5) * .5 * randomSample\n",
    "        d_sdWeight = np.matmul(np.expand_dims(hNeuron, axis=-1), np.expand_dims(d_sdBias, axis=1))\n",
    "\n",
    "        drelu = relu(hValue, derivative=True)\n",
    "        db1 = drelu * (d_muBias.dot(mu_Weight.T) + d_sdBias.dot(sd_Weight.T))\n",
    "        dW1 = np.matmul(np.expand_dims(inputs, axis=-1), np.expand_dims(db1, axis=1))        \n",
    "\n",
    "        dkl_muBias = .5 * 2 * muNeuron\n",
    "        dKL_W_mu = np.matmul(np.expand_dims(hNeuron, axis=-1), np.expand_dims(dkl_muBias, axis=1))\n",
    "\n",
    "        dkl_sdBias = .5 * (np.exp(sdNeuron) - 1)\n",
    "        dkl_sdWeight = np.matmul(np.expand_dims(hNeuron, axis=-1), np.expand_dims(dkl_sdBias, axis=1))\n",
    "\n",
    "        dkl_b1 = drelu * (dkl_sdBias.dot(sd_Weight.T) + dkl_muBias.dot(mu_Weight.T))\n",
    "        dkl_W1 = np.matmul(np.expand_dims(inputs, axis=-1), np.expand_dims(dkl_b1, axis=1))\n",
    "        \n",
    "        grad_b_logvar = dkl_sdBias + d_sdBias\n",
    "        grad_W_logvar = dkl_sdWeight + d_sdWeight\n",
    "        grad_b_mu = dkl_muBias + d_muBias\n",
    "        grad_W_mu = dkl_muWeight + d_muWeight\n",
    "        grad_b1 = dkl_b1 + db1\n",
    "        grad_W1 = dkl_W1 + dW1     \n",
    "\n",
    "        grads = [W3, b3, W2, b2, W1, b1, mu_Weight, mu_Bias, sd_Weight, sd_Bias]\n",
    "        optimise(grads)        \n",
    "\n",
    "        klbloss = -0.5 * np.sum(1 + sdNeuron - muNeuron**2 - np.exp(2 * sdNeuron)) / batch_size\n",
    "\n",
    "        #loss = -0.5 * np.sum(1 + self.latent_logvar - self.latent_mu**2 - np.exp(self.latent_logvar)) / (self.batch_size * self.latent_dim)\n",
    "        rec_loss = MSELoss(output, inputs)\n",
    "        totalLoss = rec_loss + klbloss\n",
    "        #print('---------------', index, '---------------')\n",
    "        print(totalLoss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fa9349de-d86c-4333-a920-08ec9d1c721f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3442248419.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[167], line 32\u001b[0;36m\u001b[0m\n\u001b[0;31m    error = dloss = 2 * (output - inputs)outputSig = dL * dSig\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    index = 0\n",
    "    for labels, inputs in get_training_samples(batch_size):\n",
    "        index += 1\n",
    "        #print(index)\n",
    "        inputs = np.array(inputs)\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        targets = inputs.copy()\n",
    "\n",
    "        hNeuron = sigmoid(np.dot(inputs, W1) + b1)\n",
    "\n",
    "        #print(mu_Weight.shape)\n",
    "        #muNeuron = sigmoid(np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "        #sdNeuron = sigmoid(np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "\n",
    "        muNeuron = (np.dot(hNeuron, mu_Weight) + mu_Bias)\n",
    "        sdNeuron = (np.dot(hNeuron, sd_Weight) + sd_Bias)\n",
    "        \n",
    "        #print(muNeuron.shape, batch_size)     \n",
    "        \n",
    "        randomSample = np.random.standard_normal(size=(batch_size, muNeuron.shape[1]))\n",
    "        zSpace = muNeuron + np.exp(sdNeuron * 0.5) * randomSample\n",
    "\n",
    "        # Decoder\n",
    "        hdNeuron = sigmoid(np.dot(zSpace, W2) + b2)\n",
    "        output = sigmoid(np.dot(hdNeuron, W3) + b3)\n",
    "\n",
    "        # Decoder Backpropagation\n",
    "        error = dloss = 2 * (output - inputs)outputSig = dL * dSig\n",
    "        #print(error)\n",
    "        d_output = error * output * (1 - output)\n",
    "        #d_output = error * (output > 0)\n",
    "        d_b3 = np.sum(d_output, axis=0)\n",
    "        d_W3 = np.dot(hdNeuron.T, d_output)\n",
    "        \n",
    "        d_hdNeuron = np.dot(d_output, W3.T) * hdNeuron * (1 - hdNeuron)\n",
    "        #d_hdNeuron = np.dot(d_output, W3.T) * (hdNeuron > 0 )\n",
    "        d_b2 = np.sum(d_hdNeuron, axis=0)\n",
    "        d_W2 = np.dot(zSpace.T, d_hdNeuron)\n",
    "        \n",
    "        d_zSpace = np.dot(d_hdNeuron, W2.T)\n",
    "\n",
    "\n",
    "        # Encoder Backpropagation \n",
    "        #print(d_zSpace.shape)        \n",
    "        d_muNeuron = d_zSpace\n",
    "        d_muBias = np.sum(d_muNeuron, axis = 0)\n",
    "        d_muWeight = np.dot(hNeuron.T, d_muNeuron) \n",
    "\n",
    "        d_sdNeuron = d_zSpace * np.exp(sdNeuron * .5) * .5 * randomSample\n",
    "        d_sdBias = np.sum(d_sdNeuron, axis = 0)\n",
    "        d_sdWeight = np.dot(hNeuron.T, d_sdNeuron)\n",
    "\n",
    "        hNeuronDerivative = hNeuron * (1 - hNeuron)\n",
    "        #hNeuronDerivative = (hNeuron > 0 )\n",
    "        dhNeuron = hNeuronDerivative * (np.dot(d_muBias, mu_Weight.T) + np.dot(d_sdNeuron, sd_Weight.T))  \n",
    "        dW1 = np.dot(inputs.T, dhNeuron)\n",
    "        db1 = np.sum(dhNeuron, axis = 0)\n",
    "\n",
    "        dk1_muNeuron = .5 * 2 * muNeuron\n",
    "        dkl_muBias = np.sum(dk1_muNeuron, axis = 0)\n",
    "        dkl_muWeight = np.dot(hNeuron.T, dk1_muNeuron) \n",
    "\n",
    "        dk1_sdNeuron = .5 * (np.exp(sdNeuron) - 1)\n",
    "        dkl_sdBias = np.sum(dk1_sdNeuron, axis = 0)\n",
    "        dkl_sdWeight = np.dot(hNeuron.T, dk1_sdNeuron)\n",
    "        \n",
    "        hNeuronDerivative = hNeuron * (1 - hNeuron)\n",
    "        #hNeuronDerivative = (hNeuron > 0 )\n",
    "        dkl_hNeuron = hNeuronDerivative * (np.dot(dk1_muNeuron, mu_Weight.T) + np.dot(dkl_sdBias, sd_Weight.T))\n",
    "        dkl_W1 = np.dot(inputs.T, dkl_hNeuron)\n",
    "        dkl_b1 = np.sum(dkl_hNeuron, axis = 0)\n",
    "\n",
    "        grad_b_logvar = dkl_sdBias + d_sdBias\n",
    "        grad_W_logvar = dkl_sdWeight + d_sdWeight\n",
    "        grad_b_mu = dkl_muBias + d_muBias\n",
    "        grad_W_mu = dkl_muWeight + d_muWeight\n",
    "        grad_b1 = dkl_b1 + db1\n",
    "        grad_W1 = dkl_W1 + dW1        \n",
    "\n",
    "        d_W3 = clip_gradient(d_W3)\n",
    "        d_b3 = clip_gradient(d_b3)\n",
    "        d_W2 = clip_gradient(d_W2)\n",
    "        d_b2 = clip_gradient(d_b2)\n",
    "        grad_W1 = clip_gradient(grad_W1)\n",
    "        grad_b1 = clip_gradient(grad_b1)\n",
    "        grad_W_mu = clip_gradient(grad_W_mu)\n",
    "        grad_b_mu = clip_gradient(grad_b_mu)\n",
    "        grad_W_logvar = clip_gradient(grad_W_logvar)\n",
    "        grad_b_logvar = clip_gradient(grad_b_logvar)\n",
    "\n",
    "        grads = [grad_W1, grad_b1, grad_W_mu, grad_b_mu, grad_W_logvar, grad_b_logvar, d_W2, d_b2, d_W3, d_b3]\n",
    "        optimizer = AdamOptimizer(parameters=params, learning_rate=0.001)\n",
    "        optimizer.update(params, grads)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W3 -= learning_rate * d_W3\n",
    "        b3 -= learning_rate * d_b3\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        mu_Weight -= learning_rate * grad_W_mu\n",
    "        mu_Bias -= learning_rate * grad_b_mu\n",
    "        sd_Weight -= learning_rate * grad_W_logvar\n",
    "        sd_Bias -= learning_rate * grad_b_logvar\n",
    "\n",
    "        #klbloss = -0.5 * np.sum(1 + sdNeuron - muNeuron**2 - np.exp(sdNeuron)) / (batch_size * muNeuron.shape[1])\n",
    "\n",
    "        klbloss = -0.5 * np.sum(1 + 2 * sdNeuron - muNeuron**2 - np.exp(2 * sdNeuron)) / batch_size\n",
    "\n",
    "\n",
    "        #loss = -0.5 * np.sum(1 + self.latent_logvar - self.latent_mu**2 - np.exp(self.latent_logvar)) / (self.batch_size * self.latent_dim)\n",
    "        loss = np.mean([log_loss(a, t) for a, t in zip(output, targets)])\n",
    "        totalLoss = loss + klbloss\n",
    "        print('---------------', index, '---------------')\n",
    "        #print(loss)\n",
    "        #print(klbloss)\n",
    "        print(totalLoss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8605f0-131d-40be-a9bf-7b43a9e2909f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5623b9e-d01e-45e5-8dfd-3ca70ba228a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
