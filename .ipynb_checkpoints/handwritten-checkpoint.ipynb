{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613ce9c8-c903-4c1e-91bc-db0f4e49b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_training_samples(batch_size):\n",
    "    with open(\"train.csv\") as file:\n",
    "        text = file.read()\n",
    "    textlines = text.strip().split(\"\\n\")\n",
    "    random.shuffle(textlines)\n",
    "    start = 0\n",
    "    while start < len(textlines):\n",
    "        labels = []\n",
    "        targets = []\n",
    "        inputs = []\n",
    "        end = start + batch_size\n",
    "        for textline in textlines[start:end]:\n",
    "            cells = textline.split(\",\")\n",
    "            labels.append(int(cells[0]))\n",
    "            targets.append([float(c) for c in cells[1:11]])\n",
    "            inputs.append([float(c) for c in cells[11:]])\n",
    "        yield labels, targets, inputs\n",
    "        start += batch_size\n",
    "\n",
    "def get_test_samples():\n",
    "    with open(\"test.csv\", \"r\") as file:\n",
    "        text = file.read()\n",
    "    textlines = text.strip().split(\"\\n\")\n",
    "    labels = []\n",
    "    targets = []\n",
    "    inputs = []\n",
    "    for textline in textlines:\n",
    "        cells = textline.split(\",\")\n",
    "        value = int(cells[0])\n",
    "        labels.append(int(cells[0]))\n",
    "        targets.append([float(c) for c in cells[1:11]])\n",
    "        inputs.append([float(c) for c in cells[11:]])\n",
    "    return labels, targets, inputs\n",
    "\n",
    "def plot_number(inputs):\n",
    "    line = \"\"\n",
    "    for p in inputs:\n",
    "        line += \".░▒▓█\"[round(p * 4)]\n",
    "        if len(line) > 27:\n",
    "            print(line)\n",
    "            line = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "441b29aa-5a4b-45e6-9e25-f9e89b0e8584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "620960d0-19dd-46f0-923a-803bf1eb909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(predictions):\n",
    "    m = max(predictions)\n",
    "    temp = [math.exp(p - m) for p in predictions]\n",
    "    total = sum(temp)\n",
    "    return [t / total for t in temp]\n",
    "\n",
    "#def sigmoid(x):\n",
    "#    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "    \n",
    "#def log_loss(activations, targets):\n",
    "#    losses = [-t * math.log(a) - (1 - t) * math.log(1 - a) for a, t in zip(activations, targets)]\n",
    "#    return sum(losses)\n",
    "\n",
    "def log_loss(activations, targets):\n",
    "    # Clipping values to avoid math domain error\n",
    "    clipped_activations = [max(1e-15, min(a, 1 - 1e-15)) for a in activations]\n",
    "    losses = [-t * math.log(a) - (1 - t) * math.log(1 - a) for a, t in zip(clipped_activations, targets)]\n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aae4ecbf-2736-46d5-b9c0-b27120f3c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 300\n",
    "learning_rate = 0.01\n",
    "input_count, hidden_count, output_count = 784, 260, 784\n",
    "\n",
    "w_i_h = [[random.random() - 0.5 for _ in range(input_count)] for _ in range(hidden_count)]\n",
    "w_h_o = [[random.random() - 0.5 for _ in range(hidden_count)] for _ in range(output_count)]\n",
    "b_i_h = [0 for _ in range(hidden_count)]\n",
    "b_h_o = [0 for _ in range(output_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "998d8b4c-9cb0-4385-bf0a-4bb2d4f80233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9694.815861695506\n",
      "3444.5341967283207\n",
      "3171.2498244708954\n",
      "3214.563191606346\n",
      "3183.0413944862185\n",
      "3194.019484853848\n",
      "3270.0181070872063\n",
      "3209.5654406126914\n",
      "3132.2700323220265\n",
      "3105.4493117360025\n",
      "3104.2204026707554\n",
      "3179.389156687253\n",
      "3110.428478632383\n",
      "3127.4932720402235\n",
      "3187.2033286188002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m act_h_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mactivated))\n\u001b[1;32m     49\u001b[0m errors_d_o_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39merrors_d_o))\n\u001b[0;32m---> 50\u001b[0m w_h_o_d \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merrors_d_o_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mact_h_T\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#print(w_h_o_d)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m b_h_o_d \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m deltas]) \u001b[38;5;28;01mfor\u001b[39;00m deltas \u001b[38;5;129;01min\u001b[39;00m errors_d_o_T]\n",
      "Cell \u001b[0;32mIn[42], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m act_h_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mactivated))\n\u001b[1;32m     49\u001b[0m errors_d_o_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39merrors_d_o))\n\u001b[0;32m---> 50\u001b[0m w_h_o_d \u001b[38;5;241m=\u001b[39m [\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merrors_d_o_T\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m act_h_T]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#print(w_h_o_d)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m b_h_o_d \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m deltas]) \u001b[38;5;28;01mfor\u001b[39;00m deltas \u001b[38;5;129;01min\u001b[39;00m errors_d_o_T]\n",
      "Cell \u001b[0;32mIn[42], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m act_h_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mactivated))\n\u001b[1;32m     49\u001b[0m errors_d_o_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39merrors_d_o))\n\u001b[0;32m---> 50\u001b[0m w_h_o_d \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28msum\u001b[39m([d \u001b[38;5;241m*\u001b[39m a \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(deltas, act)]) \u001b[38;5;28;01mfor\u001b[39;00m deltas \u001b[38;5;129;01min\u001b[39;00m errors_d_o_T] \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m act_h_T]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#print(w_h_o_d)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m b_h_o_d \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m deltas]) \u001b[38;5;28;01mfor\u001b[39;00m deltas \u001b[38;5;129;01min\u001b[39;00m errors_d_o_T]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for labels, targets, inputs in get_training_samples(batch_size):\n",
    "        targets = inputs.copy()\n",
    "        prediction = []\n",
    "        for inp in inputs:\n",
    "            sums = []\n",
    "            for weights, bias in zip(w_i_h, b_i_h):\n",
    "                summation = 0  # Reset summation for each neuron\n",
    "                for w, a in zip(weights, inp):\n",
    "                    summation += w * a  # Calculate weighted sum for this neuron\n",
    "                summation += bias  # Add the bias for this neuron\n",
    "                sums.append(summation)  # Append the summation result for this neuron\n",
    "            prediction.append(sums)\n",
    "\n",
    "        activated = [[max(0, val) for val in pred] for pred in prediction]\n",
    "\n",
    "        outcomes = []\n",
    "        for value in activated:\n",
    "            values = []\n",
    "            for weights, bias in zip(w_h_o, b_h_o):\n",
    "                summ = 0  # Reset summation for each neuron\n",
    "                for w, a in zip(weights, value):\n",
    "                    summ += w * a  # Calculate weighted sum for this neuron\n",
    "                summ += bias  # Add the bias for this neuron\n",
    "                values.append(summ)  # Append the summation result for this neuron\n",
    "            outcomes.append(values)\n",
    "            \n",
    "        outputs = [[max(0, value) for value in outcome] for outcome in outcomes]\n",
    "        \n",
    "        Loss = sum([log_loss(a, t) for a, t in zip(outputs, targets)]) / len(outputs)\n",
    "        \n",
    "        print(Loss)\n",
    "        \n",
    "#        errors_d_o = [[(ac - ta) * (oc * (1 - oc)) for ac, ta, oc in zip(acs, tas, outcome)]\n",
    "#                for acs, tas, outcome in zip(outputs, targets, outcomes)]\n",
    "\n",
    "        errors_d_o = [[(ac - ta) * (0 if oc <= 0 else 1)  for ac, ta, oc in zip(acs, tas, outcome)]\n",
    "                for acs, tas, outcome in zip(outputs, targets, outcomes)]\n",
    "        \n",
    "        w_h_o_T = list(zip(*w_h_o))\n",
    "        errors_d_h = [[sum([d * w for d, w in zip(deltas, weights)]) * (0 if p <= 0 else 1)\n",
    "            for weights, p in zip(w_h_o_T, pred)] for deltas, pred in zip(errors_d_o, prediction)]\n",
    "        \n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        \n",
    "        # Gradient hidden->output\n",
    "        act_h_T = list(zip(*activated))\n",
    "        errors_d_o_T = list(zip(*errors_d_o))\n",
    "        w_h_o_d = [[sum([d * a for d, a in zip(deltas, act)]) for deltas in errors_d_o_T] for act in act_h_T]\n",
    "        #print(w_h_o_d)\n",
    "        b_h_o_d = [sum([d for d in deltas]) for deltas in errors_d_o_T]\n",
    "\n",
    "        # Gradient input->hidden\n",
    "        inputs_T = list(zip(*inputs))\n",
    "        errors_d_h_T = list(zip(*errors_d_h))\n",
    "        w_i_h_d = [[sum([d * a for d, a in zip(deltas, act)]) for deltas in errors_d_h_T] for act in inputs_T]\n",
    "        b_i_h_d = [sum([d for d in deltas]) for deltas in errors_d_h_T]\n",
    "\n",
    "        # Update weights and biases for all layers\n",
    "        w_h_o_d_T = list(zip(*w_h_o_d))\n",
    "        for y in range(output_count):\n",
    "            for x in range(hidden_count):\n",
    "                w_h_o[y][x] -= learning_rate * w_h_o_d_T[y][x] / len(inputs)\n",
    "            b_h_o[y] -= learning_rate * b_h_o_d[y] / len(inputs)\n",
    "\n",
    "        w_i_h_d_T = list(zip(*w_i_h_d))\n",
    "        for y in range(hidden_count):\n",
    "            for x in range(input_count):\n",
    "                w_i_h[y][x] -= learning_rate * w_i_h_d_T[y][x] / len(inputs)\n",
    "            b_i_h[y] -= learning_rate * b_i_h_d[y] / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928c343e-5c1c-4818-a59d-4930b021d89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_238/3389222965.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     outcomes\u001b[38;5;241m.\u001b[39mappend(values)\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [[sigmoid(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m outcome] \u001b[38;5;28;01mfor\u001b[39;00m outcome \u001b[38;5;129;01min\u001b[39;00m outcomes]  \u001b[38;5;66;03m# ReLU for output layer\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m Loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(log_loss(a, t) \u001b[38;5;28;01mfor\u001b[39;00m a, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, targets)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(Loss)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate error for the output layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     outcomes\u001b[38;5;241m.\u001b[39mappend(values)\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [[sigmoid(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m outcome] \u001b[38;5;28;01mfor\u001b[39;00m outcome \u001b[38;5;129;01min\u001b[39;00m outcomes]  \u001b[38;5;66;03m# ReLU for output layer\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m Loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, targets)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(Loss)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate error for the output layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mlog_loss\u001b[0;34m(activations, targets)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_loss\u001b[39m(activations, targets):\n\u001b[0;32m---> 11\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(losses)\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_loss\u001b[39m(activations, targets):\n\u001b[0;32m---> 11\u001b[0m     losses \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39mt \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(a) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(activations, targets)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(losses)\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for labels, targets, inputs in get_training_samples(batch_size):\n",
    "        # In an autoencoder, the target is typically the input itself\n",
    "        targets = inputs.copy()\n",
    "        prediction = []\n",
    "        for inp in inputs:\n",
    "            sums = []\n",
    "            for weights, bias in zip(w_i_h, b_i_h):\n",
    "                summation = sum(w * a for w, a in zip(weights, inp)) + bias\n",
    "                sums.append(summation)\n",
    "            prediction.append(sums)\n",
    "\n",
    "        activated = [[sigmoid(val) for val in pred] for pred in prediction]  # ReLU activation\n",
    "\n",
    "        outcomes = []\n",
    "        for value in activated:\n",
    "            values = []\n",
    "            for weights, bias in zip(w_h_o, b_h_o):\n",
    "                summ = sum(w * a for w, a in zip(weights, value)) + bias\n",
    "                values.append(summ)\n",
    "            outcomes.append(values)\n",
    "\n",
    "        outputs = [[sigmoid(val) for val in outcome] for outcome in outcomes]  # ReLU for output layer\n",
    "        Loss = sum(log_loss(a, t) for a, t in zip(outputs, targets)) / len(outputs)\n",
    "        print(Loss)\n",
    "\n",
    "        # Calculate error for the output layer\n",
    "        errors_d_o = [[(ac - ta) * (0 if p <= 0 else 1) for ac, ta, outcome in zip(ac, ta, output)]\n",
    "                      for ac, ta, output in zip(outputs, targets, outcomes)]\n",
    "\n",
    "        # Transpose weight matrix for hidden to output layer\n",
    "        w_h_o_T = list(zip(*w_h_o))\n",
    "        # Calculate error for the hidden layer\n",
    "        errors_d_h = [[sum(d * w for d, w in zip(deltas, weights)) * ((0 if p <= 0 else 1))\n",
    "                       for weights, p in zip(w_h_o_T, pred)] for deltas, pred in zip(errors_d_o, prediction)]\n",
    "\n",
    "        # Calculate gradients for weights and biases\n",
    "        act_h_T = list(zip(*activated))\n",
    "        errors_d_o_T = list(zip(*errors_d_o))\n",
    "        w_h_o_d = [[sum(d * a for d, a in zip(deltas, act)) for deltas in errors_d_o_T] for act in act_h_T]\n",
    "        b_h_o_d = [sum(deltas) for deltas in errors_d_o_T]\n",
    "\n",
    "        inputs_T = list(zip(*inputs))\n",
    "        errors_d_h_T = list(zip(*errors_d_h))\n",
    "        w_i_h_d = [[sum(d * a for d, a in zip(deltas, act)) for deltas in errors_d_h_T] for act in inputs_T]\n",
    "        b_i_h_d = [sum(deltas) for deltas in errors_d_h_T]\n",
    "\n",
    "        # Update weights and biases for all layers\n",
    "        w_h_o_d_T = list(zip(*w_h_o_d))\n",
    "        for y in range(output_count):\n",
    "            for x in range(hidden_count):\n",
    "                w_h_o[y][x] -= learning_rate * w_h_o_d_T[y][x] / len(inputs)\n",
    "            b_h_o[y] -= learning_rate * b_h_o_d[y] / len(inputs)\n",
    "\n",
    "        w_i_h_d_T = list(zip(*w_i_h_d))\n",
    "        for y in range(hidden_count):\n",
    "            for x in range(input_count):\n",
    "                w_i_h[y][x] -= learning_rate * w_i_h_d_T[y][x] / len(inputs)\n",
    "            b_i_h[y] -= learning_rate * b_i_h_d[y] / len(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "d512dbae-5e8f-4d72-af84-6849fd5e89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "......░▓▒▒░░................\n",
      "......▓█████▓▓▓▓▓▓▓▓▓░......\n",
      "......░▒░▒▓██████████▒......\n",
      "............░.░░░░.██▒......\n",
      "..................░█▓.......\n",
      "..................██░.......\n",
      ".................▒██░.......\n",
      "................░██░........\n",
      "................▒█▓.........\n",
      "................▓█░.........\n",
      "...............▒█▓..........\n",
      "..............░██░..........\n",
      "..............▓█▓...........\n",
      ".............▓█▓░...........\n",
      "............░██░............\n",
      "............██▒.............\n",
      "...........▒██░.............\n",
      "..........░███░.............\n",
      "..........▒██▓░.............\n",
      "..........▒█▓...............\n",
      "............................\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[316], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs[:\u001b[38;5;241m10\u001b[39m], act_o[:\u001b[38;5;241m10\u001b[39m]):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(v)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     plot_number(v)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mplot_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[294], line 14\u001b[0m, in \u001b[0;36mplot_number\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m---> 14\u001b[0m     line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.░▒▓█\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(line)\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "labels, targets, inputs = get_test_samples()\n",
    "print(len(inputs))\n",
    "pred_h = [[sum([w * a for w, a in zip(weights, inp)]) +\n",
    "    bias for weights, bias in zip(w_i_h, b_i_h)] for inp in inputs]\n",
    "act_h = [[max(0, p) for p in pred] for pred in pred_h]\n",
    "pred_o = [[sum([w * a for w, a in zip(weights, act)]) +\n",
    "    bias for weights, bias in zip(w_h_o, b_h_o)] for act in act_h]\n",
    "act_o = [[max(0, p) for p in pred] for pred in pred_o]\n",
    "\n",
    "\n",
    "for v, i in zip(inputs[:10], act_o[:10]):\n",
    "    #print(v)\n",
    "    plot_number(v)\n",
    "    plot_number(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626b51c-fbb6-42d6-a4aa-f6a32ca225c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val, image in enumerate((inputs)):\n",
    "    plt.subplot(3, 20, i + 1)\n",
    "    plt.imshow(input.reshape(28,28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(b=False)\n",
    "plt.title('Original')    \n",
    "\n",
    "for i in range(20):\n",
    "    plt.subplot(3, 20, i + 1 + 40)\n",
    "    plt.imshow(decodtrain[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(b=False)\n",
    "  \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62f0b1-2fcd-4704-af25-cf3aa61b3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for labels, targets, inputs in get_training_samples(batch_size):       \n",
    "        pred_h = [[sum([w * a for w, a in zip(weights, inp)]) +\n",
    "            bias for weights, bias in zip(w_i_h, b_i_h)] for inp in inputs]\n",
    "        print((pred_h))\n",
    "        act_h = [[max(0, p) for p in pred] for pred in pred_h]\n",
    "        pred_o = [[sum([w * a for w, a in zip(weights, inp)]) +\n",
    "            bias for weights, bias in zip(w_h_o, b_h_o)] for inp in act_h]\n",
    "        act_o = [softmax(predictions) for predictions in pred_o]\n",
    "\n",
    "        cost = sum([log_loss(a, t) for a, t in zip(act_o, targets)]) / len(targets)\n",
    "        #print(f\"epoch:{epoch} cost:{cost:.4f}\")\n",
    "\n",
    "        # Error derivatives\n",
    "        errors_d_o = [[a - t for a, t in zip(ac, ta)] for ac, ta in zip(act_o, targets)]\n",
    "        #print(errors_d_o)\n",
    "        w_h_o_T = list(zip(*w_h_o))\n",
    "        #errors_d_h = [[print(sum([d * w for d, w in zip(deltas, weights)])) #* (0 if p <= 0 else 1)\n",
    "        #    for weights, p in zip(w_h_o_T, pred)] for deltas, pred in zip(errors_d_o, pred_h)]\n",
    "        errors_d_h = [[sum([d * w for d, w in zip(deltas, weights)]) * (0 if p <= 0 else 1)\n",
    "            for weights, p in zip(w_h_o_T, pred)] for deltas, pred in zip(errors_d_o, pred_h)]\n",
    "        print(len(w_h_o_T))\n",
    "        print('---------------------------')\n",
    "        print(len(errors_d_o))\n",
    "\n",
    "        # Gradient hidden->output\n",
    "        act_h_T = list(zip(*act_h))\n",
    "        errors_d_o_T = list(zip(*errors_d_o))\n",
    "        w_h_o_d = [[sum([d * a for d, a in zip(deltas, act)]) for deltas in errors_d_o_T] for act in act_h_T]\n",
    "        #print(w_h_o_d)\n",
    "        b_h_o_d = [sum([d for d in deltas]) for deltas in errors_d_o_T]\n",
    "\n",
    "        # Gradient input->hidden\n",
    "        inputs_T = list(zip(*inputs))\n",
    "        errors_d_h_T = list(zip(*errors_d_h))\n",
    "        w_i_h_d = [[sum([d * a for d, a in zip(deltas, act)]) for deltas in errors_d_h_T]\n",
    "            for act in inputs_T]\n",
    "        b_i_h_d = [sum([d for d in deltas]) for deltas in errors_d_h_T]\n",
    "\n",
    "        # Update weights and biases for all layers\n",
    "        w_h_o_d_T = list(zip(*w_h_o_d))\n",
    "        for y in range(output_count):\n",
    "            for x in range(hidden_count):\n",
    "                w_h_o[y][x] -= learning_rate * w_h_o_d_T[y][x] / len(inputs)\n",
    "            b_h_o[y] -= learning_rate * b_h_o_d[y] / len(inputs)\n",
    "\n",
    "        w_i_h_d_T = list(zip(*w_i_h_d))\n",
    "        for y in range(hidden_count):\n",
    "            for x in range(input_count):\n",
    "                w_i_h[y][x] -= learning_rate * w_i_h_d_T[y][x] / len(inputs)\n",
    "            b_i_h[y] -= learning_rate * b_i_h_d[y] / len(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "26c7f4c6-405e-4b42-b49d-b68e489d67f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mnistreader' has no attribute 'get_test_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m labels, targets, inputs \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_test_samples\u001b[49m()\n\u001b[1;32m      2\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28msum\u001b[39m([w \u001b[38;5;241m*\u001b[39m a \u001b[38;5;28;01mfor\u001b[39;00m w, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, inp)]) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      3\u001b[0m     bias \u001b[38;5;28;01mfor\u001b[39;00m weights, bias \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(w_i_h, b_i_h)] \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m      4\u001b[0m act_h \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pred] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m pred_h]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mnistreader' has no attribute 'get_test_samples'"
     ]
    }
   ],
   "source": [
    "labels, targets, inputs = get_test_samples()\n",
    "pred_h = [[sum([w * a for w, a in zip(weights, inp)]) +\n",
    "    bias for weights, bias in zip(w_i_h, b_i_h)] for inp in inputs]\n",
    "act_h = [[max(0, p) for p in pred] for pred in pred_h]\n",
    "pred_o = [[sum([w * a for w, a in zip(weights, act)]) +\n",
    "    bias for weights, bias in zip(w_h_o, b_h_o)] for act in act_h]\n",
    "act_o = [softmax(predictions) for predictions in pred_o]\n",
    "\n",
    "for a, t, i in zip(act_o, targets, inputs):\n",
    "    ma_neuron = a.index(max(a))\n",
    "    ma_target = t.index(max(t))\n",
    "    if ma_neuron == ma_target:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f\"digit:{ma_target}, guessed:{ma_neuron}\")\n",
    "        reader.plot_number(i)\n",
    "print(f\"Correct: {correct}/{len(inputs)} ({correct / len(inputs):%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
